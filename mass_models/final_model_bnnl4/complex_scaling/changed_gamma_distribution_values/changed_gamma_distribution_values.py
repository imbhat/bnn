# -*- coding: utf-8 -*-
"""claude_numpyro_nuclear_masses.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vtFloNEJprPjXcWKh161sh0FG9KlEoJ_
"""

# !pip install numpyro

import numpy as np
import pandas as pd
import jax.numpy as jnp
import jax
import numpyro
import numpyro.distributions as dist
from numpyro.infer import NUTS, MCMC, Predictive
from jax import random
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, QuantileTransformer
import arviz as az
import matplotlib.pyplot as plt

# Set a random seed for reproducibility
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
numpyro.set_host_device_count(4)  # Adjust based on your hardware

# Load and preprocess data
df = pd.read_csv('data.csv')  # Replace with your data loading method

target_feature = 'ws4_residuals'

input_features = ['Z', 'A', 'pairing', 'shell']

neuron_number = 28

# Split the dataset into training and validation sets (80% training, 20% validation)
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)

Z_train, A_train, Z_test, A_test = train_df['Z'], train_df['A'], val_df['Z'], val_df['A']


# Initialize scalers (fit only on the training data)
scaler_Z = StandardScaler()
scaler_A = StandardScaler()
quantile_transformer = QuantileTransformer(output_distribution='normal')

# Function to scale data (transform only, no fitting inside the function)
def scale_data(df, fit=False):
    # 1. Scaling Z and A using StandardScaler
    Z_scaled = scaler_Z.transform(df['Z'].values.reshape(-1, 1))
    A_scaled = scaler_A.transform(df['A'].values.reshape(-1, 1))

    # 2. Handling the shell feature
    # Create a binary feature for whether shell is zero or not
    shell_is_zero = (df['shell'] == 0).astype(int)

    # Extract non-zero shell values
    shell_nonzero = df['shell'][df['shell'] != 0]

    # Apply log transformation to non-zero shell values
    shell_nonzero_log = np.log(shell_nonzero)

    # Scale the log-transformed shell values using QuantileTransformer
    shell_nonzero_scaled = quantile_transformer.transform(shell_nonzero_log.values.reshape(-1, 1))

    # Reconstruct the shell feature with scaled non-zero values
    shell_scaled = pd.Series(np.zeros_like(df['shell']), index=df.index)
    shell_scaled[df['shell'] != 0] = shell_nonzero_scaled.flatten()

    # 3. Handle the pairing feature (you can use one-hot encoding if needed)
    pairing_encoded = df['pairing']  # Or pd.get_dummies(df['pairing']) for one-hot encoding

    # 4. Combine scaled features into a DataFrame
    scaled_df = pd.DataFrame({
        'Z': Z_scaled.flatten(),
        'A': A_scaled.flatten(),
        'shell': shell_scaled,
        # 'shell_is_zero': shell_is_zero,
        'pairing': pairing_encoded  # Or use one-hot encoded version
    })

    return scaled_df

# Step 1: Fit scalers on training data
scaler_Z.fit(train_df['Z'].values.reshape(-1, 1))
scaler_A.fit(train_df['A'].values.reshape(-1, 1))

# Fit the QuantileTransformer on the log-transformed non-zero shell values of the training data
shell_train_nonzero_log = np.log(train_df['shell'][train_df['shell'] != 0])
quantile_transformer.fit(shell_train_nonzero_log.values.reshape(-1, 1))

# Step 2: Apply transformations (scale) to both training and validation data
train_scaled = scale_data(train_df)
val_scaled = scale_data(val_df)

# Convert to JAX arrays
X_train_jax = jnp.array(train_scaled)
y_train_jax = jnp.array(train_df[target_feature])
X_test_jax = jnp.array(val_scaled)
y_test_jax = jnp.array(val_df[target_feature])

def bnn(X, y=None, hidden_size=neuron_number):  # Define hidden_size here
    input_size = X.shape[1]

    # Weight and bias precisions for the first layer
    fc1_weight_precision = numpyro.sample("fc1_weight_precision", dist.Gamma(2.0, 0.1))
    fc1_weight = numpyro.sample("fc1_weight", dist.Normal(0., jnp.power(fc1_weight_precision, -0.5)).expand([hidden_size, input_size]).to_event(2))
    fc1_bias_precision = numpyro.sample("fc1_bias_precision", dist.Gamma(2.0, 0.1))
    fc1_bias = numpyro.sample("fc1_bias", dist.Normal(0., jnp.power(fc1_bias_precision, -0.5)).expand([hidden_size]).to_event(1))

    # Weight and bias precisions for the second layer
    fc2_weight_precision = numpyro.sample("fc2_weight_precision", dist.Gamma(2.0, 0.1))
    fc2_weight = numpyro.sample("fc2_weight", dist.Normal(0., jnp.power(fc2_weight_precision, -0.5)).expand([1, hidden_size]).to_event(2))
    fc2_bias_precision = numpyro.sample("fc2_bias_precision", dist.Gamma(2.0, 0.1))
    fc2_bias = numpyro.sample("fc2_bias", dist.Normal(0., jnp.power(fc2_bias_precision, -0.5)).expand([1]).to_event(1))

    # Noise precision
    noise_precision = numpyro.sample("noise_precision", dist.Gamma(2.0, 0.1))

    # Model structure
    hidden = jnp.tanh(jnp.dot(X, fc1_weight.T) + fc1_bias)
    output = jnp.dot(hidden, fc2_weight.T) + fc2_bias
    output = output.squeeze()

    # Likelihood
    sigma = 1.0 / jnp.sqrt(noise_precision)
    with numpyro.plate("data", X.shape[0]):
        numpyro.sample("obs", dist.Normal(output, sigma), obs=y)
    return output

# Set up the NUTS sampler and MCMC
nuts_kernel = NUTS(bnn, target_accept_prob=0.9)
mcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000, num_chains=4)

# Run the MCMC
mcmc.run(random.PRNGKey(0), X_train_jax, y_train_jax)

# Model and chain statistics summary
idata = az.from_numpyro(mcmc)
az.summary(idata).to_csv('stat.csv')

# Extract posterior samples
posterior_samples = mcmc.get_samples()

# Save the samples (which represent your trained model parameters)
jnp.savez('ws4_residuals_model.npz', **posterior_samples)

# To load the saved parameters later
with jnp.load('ws4_residuals_model.npz') as data:
    loaded_samples = dict(data)

# Create a predictive object
predictive = Predictive(bnn, posterior_samples=loaded_samples)

# Get predictive samples for train and test sets (assuming you have them)
predictive_samples_train = predictive(jax.random.PRNGKey(0), X_train_jax)['obs']
predictive_samples_test = predictive(jax.random.PRNGKey(1), X_test_jax)['obs']

# Calculate means and standard deviations for uncertainties
mean_train = jnp.mean(predictive_samples_train, axis=0)
std_train = jnp.std(predictive_samples_train, axis=0)
mean_test = jnp.mean(predictive_samples_test, axis=0)
std_test = jnp.std(predictive_samples_test, axis=0)


# Predict the masses of the training set
predicted_mass_train = train_df['Mth_ws4'].values + mean_train

# Predict the masses of the test set
predicted_mass_test = val_df['Mth_ws4'].values + mean_test

import matplotlib.pyplot as plt

# Function to create and save residual plots
def plot_residuals(x, predicted_residuals, actual_residuals, xlabel, title, filename):
    plt.figure(figsize=(10, 6))
    
    plt.scatter(x, predicted_residuals, label="Predicted Residuals", color="blue", alpha=0.5)
    plt.scatter(x, actual_residuals, label="Actual Residuals", color="red", alpha=0.5)
    
    plt.xlabel(xlabel)
    plt.ylabel("Residuals (MeV)")
    plt.title(title)
    plt.legend()
    plt.grid(True)
    
    plt.savefig(filename, format='pdf')

# Function to create and save mass plots
def plot_masses(x, predicted_mass, actual_mass, xlabel, title, filename):
    plt.figure(figsize=(10, 6))
    
    plt.scatter(x, predicted_mass, label="Predicted Masses", color="blue", alpha=0.5)
    plt.scatter(x, actual_mass, label="Experimental Masses", color="red", alpha=0.5)
    
    plt.xlabel(xlabel)
    plt.ylabel("Mass (MeV)")
    plt.title(title)
    plt.legend()
    plt.grid(True)
    
    plt.savefig(filename, format='pdf')


# Call functions for residuals plots (Training Set)
plot_residuals(Z_train, mean_train, train_df['ws4_residuals'], 
               "Z", "Training Set: Predicted vs Actual Residuals", 
               "training_residuals_vs_Z.pdf")

plot_residuals(A_train, mean_train, train_df['ws4_residuals'], 
               "A", "Training Set: Predicted vs Actual Residuals", 
               "training_residuals_vs_A.pdf")

# Call functions for residuals plots (Validation Set)
plot_residuals(Z_test, mean_test, val_df['ws4_residuals'], 
               "Z", "Validation Set: Predicted vs Actual Residuals", 
               "test_residuals_vs_Z.pdf")

plot_residuals(A_test, mean_test, val_df['ws4_residuals'], 
               "A", "Validation Set: Predicted vs Actual Residuals", 
               "test_residuals_vs_A.pdf")


# Call functions for mass plots (Training Set)
plot_masses(Z_train, predicted_mass_train, train_df['Mexp'], 
            "Z", "Training Set: Predicted vs Experimental Masses", 
            "training_masses_vs_Z.pdf")

plot_masses(A_train, predicted_mass_train, train_df['Mexp'], 
            "A", "Training Set: Predicted vs Experimental Masses", 
            "training_masses_vs_A.pdf")

# Call functions for mass plots (Validation Set)
plot_masses(Z_test, predicted_mass_test, val_df['Mexp'], 
            "Z", "Validation Set: Predicted vs Experimental Masses", 
            "test_masses_vs_Z.pdf")

plot_masses(A_test, predicted_mass_test, val_df['Mexp'], 
            "A", "Validation Set: Predicted vs Experimental Masses", 
            "test_masses_vs_A.pdf")


# Save the RMSDs to a file
with open('output.txt', 'w') as f:
    f.write(f"Original model RMSD for training dataset was {train_df[target_feature].std()}\n")
    f.write(f"BNN-l4 model RMSD for training set is {(train_df['Mexp'] -predicted_mass_train).std()}\n")
    f.write(f"Original model RMSD for test dataset was {val_df[target_feature].std()}\n")
    f.write(f"BNN-l4 model RMSD for test set {(val_df['Mexp'] -predicted_mass_test).std()}\n")

